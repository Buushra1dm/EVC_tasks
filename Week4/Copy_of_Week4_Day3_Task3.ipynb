{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Token classification\n",
        "Find the tokens that belong to the same entity ([Chunking](https://huggingface.co/learn/nlp-course/chapter7/2?fw=pt))."
      ],
      "metadata": {
        "id": "tCgpFhPqRnvN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BJPEF3r3OOvk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f76c4163-6102-4dea-ebb6-58b6ab0eea03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load a pre-trained tokenizer and model for NER\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
        "\n",
        "# Create a NER pipeline\n",
        "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qg00hNtcZSDr",
        "outputId": "b3db415d-164e-4092-e74e-726301ec7d16"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Barack Obama was the 44th President of the United States and was born in Hawaii.\"\n",
        "\n",
        "entities = ner_pipeline(text)\n",
        "\n",
        "# Print the identified entities\n",
        "for entity in entities:\n",
        "    print(f\"Entity: {entity['word']}, Type: {entity['entity_group']}, Score: {entity['score']:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2doefVmZV_W",
        "outputId": "9d8f7a70-3d14-4e64-81fe-710683d1ddde"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity: Barack Obama, Type: PER, Score: 0.999\n",
            "Entity: United States, Type: LOC, Score: 0.997\n",
            "Entity: Hawaii, Type: LOC, Score: 0.999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def group_entities(entities):\n",
        "    grouped_entities = []\n",
        "    current_entity = {\"entity\": \"\", \"type\": \"\", \"score\": 0.0}\n",
        "\n",
        "    for entity in entities:\n",
        "        # If the current entity is empty or matches the previous type, concatenate\n",
        "        if current_entity[\"type\"] == entity[\"entity_group\"]:\n",
        "            current_entity[\"entity\"] += \" \" + entity[\"word\"]\n",
        "            current_entity[\"score\"] = max(current_entity[\"score\"], entity[\"score\"])\n",
        "        else:\n",
        "            # Append the completed entity\n",
        "            if current_entity[\"entity\"]:\n",
        "                grouped_entities.append(current_entity)\n",
        "\n",
        "            # Start a new entity\n",
        "            current_entity = {\n",
        "                \"entity\": entity[\"word\"],\n",
        "                \"type\": entity[\"entity_group\"],\n",
        "                \"score\": entity[\"score\"]\n",
        "            }\n",
        "\n",
        "    # Append the last entity\n",
        "    if current_entity[\"entity\"]:\n",
        "        grouped_entities.append(current_entity)\n",
        "\n",
        "    return grouped_entities\n",
        "\n",
        "# Group entities and print results\n",
        "grouped_entities = group_entities(entities)\n",
        "for entity in grouped_entities:\n",
        "    print(f\"Entity: {entity['entity']}, Type: {entity['type']}, Score: {entity['score']:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeFlliW0ZaIx",
        "outputId": "d110303d-7cb8-4a64-9cfa-af66293b336b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity: Barack Obama, Type: PER, Score: 0.999\n",
            "Entity: United States Hawaii, Type: LOC, Score: 0.999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "from videos"
      ],
      "metadata": {
        "id": "y4lf_zhzaOzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "model_checkpoint = \"huggingface-course/bert-finetuned-ner\"\n",
        "token_classifier = pipeline(\n",
        "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
        ")\n",
        "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBtWBaxabh8B",
        "outputId": "913bad34-fa1d-4a1f-d51f-6bbe5b443378"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'entity_group': 'PER',\n",
              "  'score': 0.9988506,\n",
              "  'word': 'Sylvain',\n",
              "  'start': 11,\n",
              "  'end': 18},\n",
              " {'entity_group': 'ORG',\n",
              "  'score': 0.9647625,\n",
              "  'word': 'Hugging Face',\n",
              "  'start': 33,\n",
              "  'end': 45},\n",
              " {'entity_group': 'LOC',\n",
              "  'score': 0.9986118,\n",
              "  'word': 'Brooklyn',\n",
              "  'start': 49,\n",
              "  'end': 57}]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_classifier(\"My name is bushra and I am AI student at jeddah\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOzBxiBTb2Y2",
        "outputId": "c1f4c3ef-719c-4d79-ceea-236db6663bbf"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'entity_group': 'ORG',\n",
              "  'score': 0.79212904,\n",
              "  'word': 'AI',\n",
              "  'start': 27,\n",
              "  'end': 29}]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_classifier(\"My name is sara and I am AI student at New York\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jd9EJsPTcKzV",
        "outputId": "2ff75154-2208-479c-8669-7c92360389d3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'entity_group': 'PER',\n",
              "  'score': 0.9493027,\n",
              "  'word': 'sa',\n",
              "  'start': 11,\n",
              "  'end': 13},\n",
              " {'entity_group': 'ORG',\n",
              "  'score': 0.62619275,\n",
              "  'word': 'AI',\n",
              "  'start': 25,\n",
              "  'end': 27},\n",
              " {'entity_group': 'LOC',\n",
              "  'score': 0.99765235,\n",
              "  'word': 'New York',\n",
              "  'start': 39,\n",
              "  'end': 47}]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S5xPlhAfcWaf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}